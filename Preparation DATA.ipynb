{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Python final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A) DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numer_sta</th>\n",
       "      <th>date</th>\n",
       "      <th>pmer</th>\n",
       "      <th>tend</th>\n",
       "      <th>cod_tend</th>\n",
       "      <th>dd</th>\n",
       "      <th>ff</th>\n",
       "      <th>t</th>\n",
       "      <th>td</th>\n",
       "      <th>u</th>\n",
       "      <th>...</th>\n",
       "      <th>hnuage1</th>\n",
       "      <th>nnuage2</th>\n",
       "      <th>ctype2</th>\n",
       "      <th>hnuage2</th>\n",
       "      <th>nnuage3</th>\n",
       "      <th>ctype3</th>\n",
       "      <th>hnuage3</th>\n",
       "      <th>nnuage4</th>\n",
       "      <th>ctype4</th>\n",
       "      <th>hnuage4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7149</td>\n",
       "      <td>2021-01-01 00:00:00</td>\n",
       "      <td>100810</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>270</td>\n",
       "      <td>1.8</td>\n",
       "      <td>272.75</td>\n",
       "      <td>272.15</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>600.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7149</td>\n",
       "      <td>2021-01-01 03:00:00</td>\n",
       "      <td>100920</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>1.7</td>\n",
       "      <td>271.25</td>\n",
       "      <td>270.95</td>\n",
       "      <td>98</td>\n",
       "      <td>...</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7149</td>\n",
       "      <td>2021-01-01 06:00:00</td>\n",
       "      <td>100950</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>290</td>\n",
       "      <td>2.6</td>\n",
       "      <td>271.95</td>\n",
       "      <td>271.65</td>\n",
       "      <td>98</td>\n",
       "      <td>...</td>\n",
       "      <td>480.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7149</td>\n",
       "      <td>2021-01-01 09:00:00</td>\n",
       "      <td>101100</td>\n",
       "      <td>150</td>\n",
       "      <td>2</td>\n",
       "      <td>280</td>\n",
       "      <td>1.7</td>\n",
       "      <td>272.45</td>\n",
       "      <td>272.05</td>\n",
       "      <td>97</td>\n",
       "      <td>...</td>\n",
       "      <td>1740.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2800.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7149</td>\n",
       "      <td>2021-01-01 12:00:00</td>\n",
       "      <td>101110</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>276.95</td>\n",
       "      <td>274.15</td>\n",
       "      <td>82</td>\n",
       "      <td>...</td>\n",
       "      <td>330.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>570.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>810.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   numer_sta                 date    pmer  tend  cod_tend   dd   ff       t  \\\n",
       "0       7149  2021-01-01 00:00:00  100810    80         1  270  1.8  272.75   \n",
       "1       7149  2021-01-01 03:00:00  100920   110         3  300  1.7  271.25   \n",
       "2       7149  2021-01-01 06:00:00  100950    30         3  290  2.6  271.95   \n",
       "3       7149  2021-01-01 09:00:00  101100   150         2  280  1.7  272.45   \n",
       "4       7149  2021-01-01 12:00:00  101110    30         0   50  1.0  276.95   \n",
       "\n",
       "       td   u  ...  hnuage1  nnuage2  ctype2  hnuage2  nnuage3  ctype3  \\\n",
       "0  272.15  96  ...    600.0      NaN     NaN      NaN      NaN     NaN   \n",
       "1  270.95  98  ...   1500.0      2.0     3.0   3000.0      NaN     NaN   \n",
       "2  271.65  98  ...    480.0      4.0     6.0   2000.0      6.0     3.0   \n",
       "3  272.05  97  ...   1740.0      3.0     3.0   2800.0      NaN     NaN   \n",
       "4  274.15  82  ...    330.0      4.0     6.0    570.0      7.0     6.0   \n",
       "\n",
       "   hnuage3  nnuage4  ctype4  hnuage4  \n",
       "0      NaN      NaN     NaN      NaN  \n",
       "1      NaN      NaN     NaN      NaN  \n",
       "2   3000.0      NaN     NaN      NaN  \n",
       "3      NaN      NaN     NaN      NaN  \n",
       "4    810.0      NaN     NaN      NaN  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import holidays\n",
    "import osmnx as ox\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "data = pd.read_parquet(Path(\"data\") / \"train.parquet\") # import train \n",
    "\n",
    "meteo = pd.read_csv(Path(\"data\") / \"external_data.csv\") # import meteo \n",
    "\n",
    "meteo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3322 entries, 0 to 3321\n",
      "Data columns (total 59 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   numer_sta  3322 non-null   int64  \n",
      " 1   date       3322 non-null   object \n",
      " 2   pmer       3322 non-null   int64  \n",
      " 3   tend       3322 non-null   int64  \n",
      " 4   cod_tend   3322 non-null   int64  \n",
      " 5   dd         3322 non-null   int64  \n",
      " 6   ff         3322 non-null   float64\n",
      " 7   t          3322 non-null   float64\n",
      " 8   td         3322 non-null   float64\n",
      " 9   u          3322 non-null   int64  \n",
      " 10  vv         3322 non-null   int64  \n",
      " 11  ww         3322 non-null   int64  \n",
      " 12  w1         3315 non-null   float64\n",
      " 13  w2         3312 non-null   float64\n",
      " 14  n          3166 non-null   float64\n",
      " 15  nbas       3317 non-null   float64\n",
      " 16  hbas       2869 non-null   float64\n",
      " 17  cl         2909 non-null   float64\n",
      " 18  cm         1941 non-null   float64\n",
      " 19  ch         1678 non-null   float64\n",
      " 20  pres       3322 non-null   int64  \n",
      " 21  niv_bar    0 non-null      float64\n",
      " 22  geop       0 non-null      float64\n",
      " 23  tend24     3312 non-null   float64\n",
      " 24  tn12       830 non-null    float64\n",
      " 25  tn24       0 non-null      float64\n",
      " 26  tx12       830 non-null    float64\n",
      " 27  tx24       0 non-null      float64\n",
      " 28  tminsol    1 non-null      float64\n",
      " 29  sw         0 non-null      float64\n",
      " 30  tw         0 non-null      float64\n",
      " 31  raf10      3312 non-null   float64\n",
      " 32  rafper     3322 non-null   float64\n",
      " 33  per        3322 non-null   int64  \n",
      " 34  etat_sol   3270 non-null   float64\n",
      " 35  ht_neige   3273 non-null   float64\n",
      " 36  ssfrai     2877 non-null   float64\n",
      " 37  perssfrai  2877 non-null   float64\n",
      " 38  rr1        3313 non-null   float64\n",
      " 39  rr3        3316 non-null   float64\n",
      " 40  rr6        3306 non-null   float64\n",
      " 41  rr12       3300 non-null   float64\n",
      " 42  rr24       3298 non-null   float64\n",
      " 43  phenspe1   0 non-null      float64\n",
      " 44  phenspe2   0 non-null      float64\n",
      " 45  phenspe3   0 non-null      float64\n",
      " 46  phenspe4   0 non-null      float64\n",
      " 47  nnuage1    2873 non-null   float64\n",
      " 48  ctype1     2524 non-null   float64\n",
      " 49  hnuage1    2867 non-null   float64\n",
      " 50  nnuage2    1695 non-null   float64\n",
      " 51  ctype2     1443 non-null   float64\n",
      " 52  hnuage2    1695 non-null   float64\n",
      " 53  nnuage3    618 non-null    float64\n",
      " 54  ctype3     470 non-null    float64\n",
      " 55  hnuage3    618 non-null    float64\n",
      " 56  nnuage4    42 non-null     float64\n",
      " 57  ctype4     87 non-null     float64\n",
      " 58  hnuage4    42 non-null     float64\n",
      "dtypes: float64(48), int64(10), object(1)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "meteo.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A-1) DONNEE METEO   \n",
    "ETAPE 1: Filtrer les données (date et geographie et NaN)\n",
    "\n",
    "1/ FILTRER PAR DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meteo dataset: 2021-10-21 12:00:00 2020-09-01 00:00:00\n",
      "Data dataset: 2021-09-09 23:00:00 2020-09-01 01:00:00\n"
     ]
    }
   ],
   "source": [
    "# DATE \n",
    "\n",
    "# Convertir les colonne en type datetime\n",
    "meteo['date'] = pd.to_datetime(meteo['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Trouver la date minimum et maximum de meteo et data\n",
    "date_min_meteo = meteo['date'].min()\n",
    "date_max_meteo = meteo['date'].max()\n",
    "date_min_data = data['date'].min()\n",
    "date_max_data = data['date'].max()\n",
    "\n",
    "print(\"Meteo dataset:\", date_max_meteo, date_min_meteo)\n",
    "print(\"Data dataset:\", date_max_data, date_min_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donc, ces données overlap nos training, mais sont dans le meme format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtrer\n",
    "meteo = meteo[(meteo['date'] >= date_min_data) & (meteo['date'] <= date_max_data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2/ FILTRER PAR GEOGRAPHIE \n",
    "\n",
    "On remarque qu'il n'y a aucune indication sur la geographie (region, departement, commune, longitude ou latitude)\n",
    "DONC, on va HYPOTHESE IMPORTANTE: supposer que toutes les données concernent paris intra-muros (ou sont localisé nos bornes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3/ GERER LES VALEURS MANQUANTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lignes: 2988 colonnes: 59\n"
     ]
    }
   ],
   "source": [
    "# Gerer les valeurs manquantes\n",
    "print(\"Lignes:\", meteo.shape[0], \"colonnes:\", meteo.shape[1]) # trouver le nombre de lignes dans le dataset filtré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numer_sta       0\n",
       "date            0\n",
       "pmer            0\n",
       "tend            0\n",
       "cod_tend        0\n",
       "dd              0\n",
       "ff              0\n",
       "t               0\n",
       "td              0\n",
       "u               0\n",
       "vv              0\n",
       "ww              0\n",
       "w1              7\n",
       "w2             10\n",
       "n             138\n",
       "nbas            5\n",
       "hbas          410\n",
       "cl            371\n",
       "cm           1284\n",
       "ch           1526\n",
       "pres            0\n",
       "niv_bar      2988\n",
       "geop         2988\n",
       "tend24         10\n",
       "tn12         2241\n",
       "tn24         2988\n",
       "tx12         2241\n",
       "tx24         2988\n",
       "tminsol      2987\n",
       "sw           2988\n",
       "tw           2988\n",
       "raf10          10\n",
       "rafper          0\n",
       "per             0\n",
       "etat_sol       49\n",
       "ht_neige       44\n",
       "ssfrai        403\n",
       "perssfrai     403\n",
       "rr1             8\n",
       "rr3             4\n",
       "rr6            12\n",
       "rr12           16\n",
       "rr24           18\n",
       "phenspe1     2988\n",
       "phenspe2     2988\n",
       "phenspe3     2988\n",
       "phenspe4     2988\n",
       "nnuage1       406\n",
       "ctype1        720\n",
       "hnuage1       412\n",
       "nnuage2      1466\n",
       "ctype2       1691\n",
       "hnuage2      1466\n",
       "nnuage3      2445\n",
       "ctype3       2577\n",
       "hnuage3      2445\n",
       "nnuage4      2954\n",
       "ctype4       2914\n",
       "hnuage4      2954\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meteo.isna().sum() # Savoir le nombre de Nan dans chaque colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lignes: 2988 colonnes: 49\n",
      "Lignes: 2988 colonnes: 48\n"
     ]
    }
   ],
   "source": [
    "# Ensuite, on retire toutes les colonnes qui sont remplis en totalité de valeurs manquantes\n",
    "meteo = meteo.dropna(axis=1, how='all').copy()\n",
    "print(\"Lignes:\", meteo.shape[0], \"colonnes:\", meteo.shape[1]) # trouver le nombre de lignes dans le dataset filtré\n",
    "\n",
    "# On a egalement remarque que dabs 'Tminsol' il n'y a qu'une valeur, on peut donc egalement supprimer cette colonne\n",
    "meteo = meteo.drop('tminsol', axis=1).copy()\n",
    "print(\"Lignes:\", meteo.shape[0], \"colonnes:\", meteo.shape[1]) # check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4/ GERER LES COLONNES A MODALITE UNIQUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numer_sta       1\n",
       "date         2987\n",
       "pmer          461\n",
       "tend           97\n",
       "cod_tend        9\n",
       "dd             37\n",
       "ff            106\n",
       "t             345\n",
       "td            277\n",
       "u              77\n",
       "vv           1137\n",
       "ww             42\n",
       "w1              9\n",
       "w2              9\n",
       "n              10\n",
       "nbas           10\n",
       "hbas           91\n",
       "cl             11\n",
       "cm              9\n",
       "ch             10\n",
       "pres          458\n",
       "tend24        340\n",
       "tn12          215\n",
       "tx12          276\n",
       "raf10         168\n",
       "rafper        182\n",
       "per             1\n",
       "etat_sol        9\n",
       "ht_neige        7\n",
       "ssfrai          3\n",
       "perssfrai       2\n",
       "rr1            31\n",
       "rr3            58\n",
       "rr6            84\n",
       "rr12          118\n",
       "rr24          152\n",
       "nnuage1         9\n",
       "ctype1          8\n",
       "hnuage1       270\n",
       "nnuage2         8\n",
       "ctype2          8\n",
       "hnuage2       235\n",
       "nnuage3         8\n",
       "ctype3          8\n",
       "hnuage3       152\n",
       "nnuage4         7\n",
       "ctype4          5\n",
       "hnuage4        24\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meteo.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2988 entries, 0 to 3321\n",
      "Data columns (total 46 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   date       2988 non-null   datetime64[ns]\n",
      " 1   pmer       2988 non-null   int64         \n",
      " 2   tend       2988 non-null   int64         \n",
      " 3   cod_tend   2988 non-null   int64         \n",
      " 4   dd         2988 non-null   int64         \n",
      " 5   ff         2988 non-null   float64       \n",
      " 6   t          2988 non-null   float64       \n",
      " 7   td         2988 non-null   float64       \n",
      " 8   u          2988 non-null   int64         \n",
      " 9   vv         2988 non-null   int64         \n",
      " 10  ww         2988 non-null   int64         \n",
      " 11  w1         2981 non-null   float64       \n",
      " 12  w2         2978 non-null   float64       \n",
      " 13  n          2850 non-null   float64       \n",
      " 14  nbas       2983 non-null   float64       \n",
      " 15  hbas       2578 non-null   float64       \n",
      " 16  cl         2617 non-null   float64       \n",
      " 17  cm         1704 non-null   float64       \n",
      " 18  ch         1462 non-null   float64       \n",
      " 19  pres       2988 non-null   int64         \n",
      " 20  tend24     2978 non-null   float64       \n",
      " 21  tn12       747 non-null    float64       \n",
      " 22  tx12       747 non-null    float64       \n",
      " 23  raf10      2978 non-null   float64       \n",
      " 24  rafper     2988 non-null   float64       \n",
      " 25  etat_sol   2939 non-null   float64       \n",
      " 26  ht_neige   2944 non-null   float64       \n",
      " 27  ssfrai     2585 non-null   float64       \n",
      " 28  perssfrai  2585 non-null   float64       \n",
      " 29  rr1        2980 non-null   float64       \n",
      " 30  rr3        2984 non-null   float64       \n",
      " 31  rr6        2976 non-null   float64       \n",
      " 32  rr12       2972 non-null   float64       \n",
      " 33  rr24       2970 non-null   float64       \n",
      " 34  nnuage1    2582 non-null   float64       \n",
      " 35  ctype1     2268 non-null   float64       \n",
      " 36  hnuage1    2576 non-null   float64       \n",
      " 37  nnuage2    1522 non-null   float64       \n",
      " 38  ctype2     1297 non-null   float64       \n",
      " 39  hnuage2    1522 non-null   float64       \n",
      " 40  nnuage3    543 non-null    float64       \n",
      " 41  ctype3     411 non-null    float64       \n",
      " 42  hnuage3    543 non-null    float64       \n",
      " 43  nnuage4    34 non-null     float64       \n",
      " 44  ctype4     74 non-null     float64       \n",
      " 45  hnuage4    34 non-null     float64       \n",
      "dtypes: datetime64[ns](1), float64(37), int64(8)\n",
      "memory usage: 1.1 MB\n"
     ]
    }
   ],
   "source": [
    "meteo = meteo.loc[:, meteo.nunique() > 1] # supprimer colonnes à modalité unique\n",
    "meteo.info() # check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5/ GERER LES DUPLICATS\n",
    "\n",
    "On remarque que le dataset a 2988 observations, hors il n'y a que 2987 modalités pour la date. Cela signifie qu'il y a un doublon. Verifions cela. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    date    pmer  tend  cod_tend   dd   ff       t      td  \\\n",
      "2017 2020-11-20 18:00:00  103650    20         2  180  1.0  278.15  276.45   \n",
      "2018 2020-11-20 18:00:00  103650    20         2  180  1.0  278.15  276.45   \n",
      "\n",
      "       u     vv  ...  hnuage1  nnuage2  ctype2  hnuage2  nnuage3  ctype3  \\\n",
      "2017  89  25000  ...   7000.0      NaN     NaN      NaN      NaN     NaN   \n",
      "2018  89  25000  ...   7000.0      NaN     NaN      NaN      NaN     NaN   \n",
      "\n",
      "      hnuage3  nnuage4  ctype4  hnuage4  \n",
      "2017      NaN      NaN     NaN      NaN  \n",
      "2018      NaN      NaN     NaN      NaN  \n",
      "\n",
      "[2 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "duplicates = meteo[meteo['date'].duplicated(keep=False)]  # selectionner le doublons\n",
    "print(duplicates) # Afficher les doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfectly identical observations?  True\n"
     ]
    }
   ],
   "source": [
    "# Il y a donc bien un doublon concernant la date. Verifions si ce sont des doublons \"parfaits\"\n",
    "identiques = duplicates.iloc[0].equals(duplicates.iloc[1]) # verifier l'egalité element par element\n",
    "print(\"Perfectly identical observations? \", identiques) # resultats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut donc en supprimer un \n",
    "meteo = meteo.drop_duplicates(subset=['date'], keep='first') # garder que la 1ere occurence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETAPE 2: Comprendre les données et les grouper en thèmes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2987 entries, 0 to 3321\n",
      "Data columns (total 46 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   date       2987 non-null   datetime64[ns]\n",
      " 1   pmer       2987 non-null   int64         \n",
      " 2   tend       2987 non-null   int64         \n",
      " 3   cod_tend   2987 non-null   int64         \n",
      " 4   dd         2987 non-null   int64         \n",
      " 5   ff         2987 non-null   float64       \n",
      " 6   t          2987 non-null   float64       \n",
      " 7   td         2987 non-null   float64       \n",
      " 8   u          2987 non-null   int64         \n",
      " 9   vv         2987 non-null   int64         \n",
      " 10  ww         2987 non-null   int64         \n",
      " 11  w1         2980 non-null   float64       \n",
      " 12  w2         2977 non-null   float64       \n",
      " 13  n          2849 non-null   float64       \n",
      " 14  nbas       2982 non-null   float64       \n",
      " 15  hbas       2577 non-null   float64       \n",
      " 16  cl         2616 non-null   float64       \n",
      " 17  cm         1703 non-null   float64       \n",
      " 18  ch         1461 non-null   float64       \n",
      " 19  pres       2987 non-null   int64         \n",
      " 20  tend24     2977 non-null   float64       \n",
      " 21  tn12       746 non-null    float64       \n",
      " 22  tx12       746 non-null    float64       \n",
      " 23  raf10      2977 non-null   float64       \n",
      " 24  rafper     2987 non-null   float64       \n",
      " 25  etat_sol   2938 non-null   float64       \n",
      " 26  ht_neige   2943 non-null   float64       \n",
      " 27  ssfrai     2584 non-null   float64       \n",
      " 28  perssfrai  2584 non-null   float64       \n",
      " 29  rr1        2979 non-null   float64       \n",
      " 30  rr3        2983 non-null   float64       \n",
      " 31  rr6        2975 non-null   float64       \n",
      " 32  rr12       2971 non-null   float64       \n",
      " 33  rr24       2969 non-null   float64       \n",
      " 34  nnuage1    2581 non-null   float64       \n",
      " 35  ctype1     2267 non-null   float64       \n",
      " 36  hnuage1    2575 non-null   float64       \n",
      " 37  nnuage2    1522 non-null   float64       \n",
      " 38  ctype2     1297 non-null   float64       \n",
      " 39  hnuage2    1522 non-null   float64       \n",
      " 40  nnuage3    543 non-null    float64       \n",
      " 41  ctype3     411 non-null    float64       \n",
      " 42  hnuage3    543 non-null    float64       \n",
      " 43  nnuage4    34 non-null     float64       \n",
      " 44  ctype4     74 non-null     float64       \n",
      " 45  hnuage4    34 non-null     float64       \n",
      "dtypes: datetime64[ns](1), float64(37), int64(8)\n",
      "memory usage: 1.1 MB\n"
     ]
    }
   ],
   "source": [
    "meteo.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va creer des groupes de données par thèmes (etant donné que la variable \"Date\" est celle qui va nous permettre de faire le lien avec notre training dataset on la mets partout)\n",
    "1) Pression\n",
    "2) Nuage (et visibilité)\n",
    "3) Precipitations\n",
    "4) Vent\n",
    "5) Temperature\n",
    "6) Neige\n",
    "7) Humidité\n",
    "8) Etat du sol (1 donnée)\n",
    "9) Autre (temps present, temps passé 1 et 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation des 9 sous-ensembles\n",
    "\n",
    "# 1) Pression\n",
    "pression_columns = ['date', 'pmer', 'tend', 'pres', 'tend24', 'cod_tend']  \n",
    "pression  = meteo[pression_columns]\n",
    "\n",
    "# 2) Nuage (et visibilité)\n",
    "nuage_columns = ['date', 'vv', 'nbas', 'hbas', 'cl', 'cm', 'ch', 'n', 'nnuage1', 'ctype1',  'hnuage1', 'nnuage2', 'ctype2', 'hnuage2', 'nnuage3', 'ctype3', 'hnuage3', 'nnuage4', 'ctype4', 'hnuage4'] \n",
    "nuage = meteo[nuage_columns]\n",
    "\n",
    "# 3) Précipitations\n",
    "precip_columns = ['date', 'rr1', 'rr12', 'rr3', 'rr6', 'rr24']  \n",
    "precip = meteo[precip_columns]\n",
    "\n",
    "# 4) Vent\n",
    "vent_columns = ['date', 'dd', 'ff', 'rafper', 'raf10']  \n",
    "vent = meteo[vent_columns]\n",
    "\n",
    "# 5) Température\n",
    "temp_columns = ['date', 't', 'tn12', 'tx12']  \n",
    "temp = meteo[temp_columns]\n",
    "\n",
    "# 6) Neige\n",
    "neige_columns = ['date', 'ssfrai', 'perssfrai', 'ht_neige']  \n",
    "neige = meteo[neige_columns]\n",
    "\n",
    "# 7) Humidité\n",
    "humidite_columns = ['date', 'u', 'td'] \n",
    "humidite = meteo[humidite_columns]\n",
    "\n",
    "# 8) Etat du sol (1 donnée)\n",
    "etat_sol_columns = ['date', 'etat_sol'] \n",
    "sol = meteo[etat_sol_columns]\n",
    "\n",
    "# 9) Autre (temps présent, temps passé 1 et 2)\n",
    "autre_columns = ['date', 'ww', 'w1', 'w2']  # Autres informations liées au temps passé et présent\n",
    "autre = meteo[autre_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Nombre de colonnes dans le dataset initial en excluant la colonne 'date'\n",
    "initial_columns = len(meteo.columns) - 1  # Exclure 'date'\n",
    "\n",
    "# Nombre total de colonnes dans les sous-ensembles\n",
    "total_subsets_columns = (\n",
    "    len(pression_columns) + \n",
    "    len(nuage_columns) + \n",
    "    len(precip_columns) + \n",
    "    len(vent_columns) + \n",
    "    len(temp_columns) + \n",
    "    len(neige_columns) + \n",
    "    len(humidite_columns) + \n",
    "    len(etat_sol_columns) + \n",
    "    len(autre_columns) - 9  # Exclure la colonne 'date' dans chaque sous-ensemble\n",
    ")\n",
    "\n",
    "# Vérification si la somme des colonnes dans les sous-ensembles est égale au nombre de colonnes initial\n",
    "if initial_columns == total_subsets_columns:\n",
    "    print(\"OK\")\n",
    "else:\n",
    "    print(\"Problem\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A-2) DONNEE TRAIN   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data:\n",
      " counter_id                   0\n",
      "counter_name                 0\n",
      "site_id                      0\n",
      "site_name                    0\n",
      "bike_count                   0\n",
      "date                         0\n",
      "counter_installation_date    0\n",
      "coordinates                  0\n",
      "counter_technical_id         0\n",
      "latitude                     0\n",
      "longitude                    0\n",
      "log_bike_count               0\n",
      "dtype: int64\n",
      "Number of duplicates before removal: 0\n",
      "Number of duplicates after removal: 0\n"
     ]
    }
   ],
   "source": [
    "#Cleaning\n",
    "\n",
    "# 1. Handle Missing Data\n",
    "# Check for missing values\n",
    "missing_data = data.isnull().sum()\n",
    "print(\"Missing data:\\n\", missing_data)\n",
    "\n",
    "# Drop rows with missing values in crucial columns\n",
    "data = data.dropna(subset=['bike_count', 'date', 'latitude', 'longitude'])\n",
    "\n",
    "# 3. Remove Duplicates\n",
    "print(\"Number of duplicates before removal:\", data.duplicated().sum())\n",
    "data = data.drop_duplicates(subset=['counter_id', 'date'])\n",
    "print(\"Number of duplicates after removal:\", data.duplicated().sum())\n",
    "\n",
    "# 4. Handle Outliers (check negative values in bike_count)\n",
    "if (data['bike_count'] < 0).any():\n",
    "    print(\"Negative bike counts detected.\")\n",
    "    data['bike_count'] = data['bike_count'].clip(lower=0)  # Replace negative values with 0\n",
    "\n",
    "# 5. Validate Geographic Coordinates\n",
    "# Check for invalid latitude and longitude values\n",
    "invalid_coords = data[(data['latitude'] < -90) | (data['latitude'] > 90) |\n",
    "                      (data['longitude'] < -180) | (data['longitude'] > 180)]\n",
    "if not invalid_coords.empty:\n",
    "    print(f\"Invalid coordinates detected:\\n{invalid_coords}\")\n",
    "    data = data.dropna(subset=['latitude', 'longitude'])  # Optionally drop invalid coordinates\n",
    "\n",
    "# 6. Clean Text Columns (remove extra spaces, convert to lowercase)\n",
    "text_columns = ['counter_name', 'site_name', 'counter_technical_id']\n",
    "for col in text_columns:\n",
    "    data[col] = data[col].str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "counter_id                     56\n",
       "counter_name                   56\n",
       "site_id                        30\n",
       "site_name                      30\n",
       "bike_count                    998\n",
       "date                         8974\n",
       "counter_installation_date      22\n",
       "coordinates                    30\n",
       "counter_technical_id           30\n",
       "latitude                       30\n",
       "longitude                      30\n",
       "log_bike_count                998\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. modalités uniques\n",
    "data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counter_id</th>\n",
       "      <th>counter_name</th>\n",
       "      <th>site_id</th>\n",
       "      <th>site_name</th>\n",
       "      <th>bike_count</th>\n",
       "      <th>date</th>\n",
       "      <th>counter_installation_date</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>counter_technical_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>...</th>\n",
       "      <th>date_only</th>\n",
       "      <th>hour_only</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_rush_hour</th>\n",
       "      <th>season</th>\n",
       "      <th>vacation</th>\n",
       "      <th>is_public_holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48321</th>\n",
       "      <td>100007049-102007049</td>\n",
       "      <td>28 boulevard diderot e-o</td>\n",
       "      <td>100007049</td>\n",
       "      <td>28 boulevard diderot</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-09-01 02:00:00</td>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>48.846028,2.375429</td>\n",
       "      <td>y2h15027244</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>autumn</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48324</th>\n",
       "      <td>100007049-102007049</td>\n",
       "      <td>28 boulevard diderot e-o</td>\n",
       "      <td>100007049</td>\n",
       "      <td>28 boulevard diderot</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-09-01 03:00:00</td>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>48.846028,2.375429</td>\n",
       "      <td>y2h15027244</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>autumn</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48327</th>\n",
       "      <td>100007049-102007049</td>\n",
       "      <td>28 boulevard diderot e-o</td>\n",
       "      <td>100007049</td>\n",
       "      <td>28 boulevard diderot</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-09-01 04:00:00</td>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>48.846028,2.375429</td>\n",
       "      <td>y2h15027244</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>autumn</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48330</th>\n",
       "      <td>100007049-102007049</td>\n",
       "      <td>28 boulevard diderot e-o</td>\n",
       "      <td>100007049</td>\n",
       "      <td>28 boulevard diderot</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2020-09-01 15:00:00</td>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>48.846028,2.375429</td>\n",
       "      <td>y2h15027244</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>autumn</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48333</th>\n",
       "      <td>100007049-102007049</td>\n",
       "      <td>28 boulevard diderot e-o</td>\n",
       "      <td>100007049</td>\n",
       "      <td>28 boulevard diderot</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2020-09-01 18:00:00</td>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>48.846028,2.375429</td>\n",
       "      <td>y2h15027244</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>autumn</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                counter_id              counter_name    site_id  \\\n",
       "48321  100007049-102007049  28 boulevard diderot e-o  100007049   \n",
       "48324  100007049-102007049  28 boulevard diderot e-o  100007049   \n",
       "48327  100007049-102007049  28 boulevard diderot e-o  100007049   \n",
       "48330  100007049-102007049  28 boulevard diderot e-o  100007049   \n",
       "48333  100007049-102007049  28 boulevard diderot e-o  100007049   \n",
       "\n",
       "                  site_name  bike_count                date  \\\n",
       "48321  28 boulevard diderot         0.0 2020-09-01 02:00:00   \n",
       "48324  28 boulevard diderot         1.0 2020-09-01 03:00:00   \n",
       "48327  28 boulevard diderot         0.0 2020-09-01 04:00:00   \n",
       "48330  28 boulevard diderot         4.0 2020-09-01 15:00:00   \n",
       "48333  28 boulevard diderot         9.0 2020-09-01 18:00:00   \n",
       "\n",
       "      counter_installation_date         coordinates counter_technical_id  \\\n",
       "48321                2013-01-18  48.846028,2.375429          y2h15027244   \n",
       "48324                2013-01-18  48.846028,2.375429          y2h15027244   \n",
       "48327                2013-01-18  48.846028,2.375429          y2h15027244   \n",
       "48330                2013-01-18  48.846028,2.375429          y2h15027244   \n",
       "48333                2013-01-18  48.846028,2.375429          y2h15027244   \n",
       "\n",
       "        latitude  ...   date_only  hour_only day_of_week  month  year  \\\n",
       "48321  48.846028  ...  2020-09-01          2           1      9  2020   \n",
       "48324  48.846028  ...  2020-09-01          3           1      9  2020   \n",
       "48327  48.846028  ...  2020-09-01          4           1      9  2020   \n",
       "48330  48.846028  ...  2020-09-01         15           1      9  2020   \n",
       "48333  48.846028  ...  2020-09-01         18           1      9  2020   \n",
       "\n",
       "       is_weekend  is_rush_hour  season  vacation is_public_holiday  \n",
       "48321           0             0  autumn         0                 0  \n",
       "48324           0             0  autumn         0                 0  \n",
       "48327           0             0  autumn         0                 0  \n",
       "48330           0             0  autumn         0                 0  \n",
       "48333           0             1  autumn         0                 0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Convert `date` column to datetime type\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# 2. Extract temporal components\n",
    "data['date_only'] = data['date'].dt.date\n",
    "data['hour_only'] = data['date'].dt.hour\n",
    "data['day_of_week'] = data['date'].dt.dayofweek\n",
    "data['month'] = data['date'].dt.month\n",
    "data['year'] = data['date'].dt.year\n",
    "\n",
    "# 3. Add temporal indicators\n",
    "data['is_weekend'] = data['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "data['is_rush_hour'] = data['hour_only'].apply(lambda x: 1 if (7 <= x <= 9 or 17 <= x <= 19) else 0)\n",
    "data['season'] = data['month'].apply(lambda x: 'winter' if x in [12, 1, 2] \\\n",
    "                                      else 'spring' if x in [3, 4, 5] \\\n",
    "                                      else 'summer' if x in [6, 7, 8] \\\n",
    "                                      else 'autumn')\n",
    "\n",
    "# 4. Add holiday indicators\n",
    "def vacation(date):\n",
    "    summer_vacation = (date.month == 7 or date.month == 8)\n",
    "    christmas_vacation = (date.month == 12 and date.day >= 20)\n",
    "    return 1 if (summer_vacation or christmas_vacation) else 0\n",
    "\n",
    "data['vacation'] = data['date'].apply(vacation)\n",
    "\n",
    "# 5. Add public holidays using the holidays library\n",
    "fr_holidays = holidays.France(years=data['year'].unique().tolist())\n",
    "def is_public_holiday(date):\n",
    "    return 1 if date in fr_holidays else 0\n",
    "\n",
    "data['is_public_holiday'] = data['date'].apply(is_public_holiday)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 496827 entries, 48321 to 929187\n",
      "Data columns (total 22 columns):\n",
      " #   Column                     Non-Null Count   Dtype         \n",
      "---  ------                     --------------   -----         \n",
      " 0   counter_id                 496827 non-null  category      \n",
      " 1   counter_name               496827 non-null  object        \n",
      " 2   site_id                    496827 non-null  int64         \n",
      " 3   site_name                  496827 non-null  object        \n",
      " 4   bike_count                 496827 non-null  float64       \n",
      " 5   date                       496827 non-null  datetime64[us]\n",
      " 6   counter_installation_date  496827 non-null  datetime64[us]\n",
      " 7   coordinates                496827 non-null  category      \n",
      " 8   counter_technical_id       496827 non-null  object        \n",
      " 9   latitude                   496827 non-null  float64       \n",
      " 10  longitude                  496827 non-null  float64       \n",
      " 11  log_bike_count             496827 non-null  float64       \n",
      " 12  date_only                  496827 non-null  object        \n",
      " 13  hour_only                  496827 non-null  int32         \n",
      " 14  day_of_week                496827 non-null  int32         \n",
      " 15  month                      496827 non-null  int32         \n",
      " 16  year                       496827 non-null  int32         \n",
      " 17  is_weekend                 496827 non-null  int64         \n",
      " 18  is_rush_hour               496827 non-null  int64         \n",
      " 19  season                     496827 non-null  object        \n",
      " 20  vacation                   496827 non-null  int64         \n",
      " 21  is_public_holiday          496827 non-null  int64         \n",
      "dtypes: category(2), datetime64[us](2), float64(4), int32(4), int64(5), object(5)\n",
      "memory usage: 73.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features (X) and response variable (y)\n",
    "X = data.drop(columns=['bike_count', 'log_bike_count', 'counter_id', 'site_id', 'coordinates', 'counter_technical_id', 'date_only']) \n",
    "y = data['log_bike_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) MODELES\n",
    "\n",
    "Modèles testés:  \n",
    "- Decision tree regressor\n",
    "- Random forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from dirty_cat import TableVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 1: Creer toutes les combinaisons possibles de features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generer toutes les combinaisons possible de features optionelles\n",
    "from itertools import combinations\n",
    "\n",
    "# features obligatoires\n",
    "mandatory_features = X[['counter_name', 'site_name', 'date', 'counter_installation_date', 'latitude', 'longitude']]\n",
    "\n",
    "# features optionelles\n",
    "X_optional = X.drop(columns=['counter_name', 'site_name', 'counter_installation_date', 'latitude', 'longitude']) # sauf date car c'est ce qui permet de merge, retirer \n",
    "optional_datasets = [pression, nuage, precip, vent, temp, neige, humidite, sol, autre, X_optional]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'AI EXCLUT TOUT CE QUI ETAIT METEO POUR L'INSTANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rk/666lrzxx1c5gqws88bk4jq380000gn/T/ipykernel_97047/2120478895.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Exclure la colonne \"date\" de X_optional pour les combinaisons de colonnes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0moptional_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_optional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0moptional_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mall_combinations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptional_columns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# On inclut toutes les tailles de combinaisons possibles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcombo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptional_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bike_counters-AM-/env/lib/python3.8/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mindicator\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m ) -> DataFrame:\n\u001b[0;32m--> 148\u001b[0;31m     op = _MergeOperation(\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bike_counters-AM-/env/lib/python3.8/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    733\u001b[0m         (\n\u001b[1;32m    734\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0;31m# to avoid incompatible dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bike_counters-AM-/env/lib/python3.8/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1199\u001b[0m                         \u001b[0;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m                         \u001b[0;31m#  the latter of which will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m                         \u001b[0mrk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m                             \u001b[0;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bike_counters-AM-/env/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1774\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1776\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1781\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'date'"
     ]
    }
   ],
   "source": [
    "# Exclure la colonne \"date\" de X_optional pour les combinaisons de colonnes\n",
    "optional_columns = X_optional.columns.tolist()\n",
    "optional_columns.remove('date')\n",
    "\n",
    "all_combinations = []\n",
    "for i in range(1, len(optional_columns) + 1):  # On inclut toutes les tailles de combinaisons possibles\n",
    "    for combo in combinations(optional_columns, i):\n",
    "        # Créer un nouveau DataFrame avec la colonne \"date\" et la combinaison de colonnes\n",
    "        option = pd.merge(mandatory_features, X_optional[list(combo)], on='date')\n",
    "        all_combinations.append(option)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B-1) Decision tree regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 2: Encoding: Table Vectorizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 3: Define the model and tune parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
